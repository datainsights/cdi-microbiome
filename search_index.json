[["index.html", "Unlocking Microbial Insights ", " Unlocking Microbial Insights Last updated: June 29, 2025 "],["welcome-to-cdi-unlocking-microbial-insights.html", "Welcome to CDI ‚Äì Unlocking Microbial Insights", " Welcome to CDI ‚Äì Unlocking Microbial Insights Welcome to the Microbiome Data Science Q&amp;A Guide, part of the Complex Data Insights (CDI) open learning series. This guide helps you make sense of microbial abundance, diversity, and metadata ‚Äî all through a practical, question-driven approach. Whether you‚Äôre a student, researcher, or data enthusiast, you‚Äôll explore key concepts step by step using real microbial datasets and modern tools. Let curiosity guide you ‚Äî and let the data speak. üåê The CDI Learning Path This guide is part of the Complex Data Insights (CDI) learning system ‚Äî a fully free and open-source project licensed under the MIT License. CDI breaks down complex topics into four progressive layers, designed to be explored individually or as an integrated journey: üîç EDA (Exploratory Data Analysis) Understand your data ‚Äî explore its structure, patterns, and quirks. üìä VIZ (Visualization) Communicate findings through clear and compelling visuals. üìê STATS (Statistical Analysis) Test hypotheses and quantify uncertainty using sound statistical methods. ü§ñ ML (Machine Learning) Build models to predict, classify, and uncover deeper insights. CDI helps you grow ‚Äî one Q&amp;A at a time. "],["what-are-the-essential-tools-for-microbiome-read-quality-control.html", "Q&A 1 What are the essential tools for microbiome read quality control? 1.1 Explanation 1.2 Shell Code 1.3 R Note", " Q&A 1 What are the essential tools for microbiome read quality control? 1.1 Explanation Every microbiome analysis begins with raw sequencing data, often in the form of FASTQ files. These files contain both the nucleotide reads and quality scores. However, raw reads are rarely perfect ‚Äî they may contain adapter sequences, low-quality regions, or even contaminant DNA. Before proceeding to any taxonomic or functional profiling, it‚Äôs essential to clean and assess these reads. This is the foundation of your analysis pipeline ‚Äî ensuring that only high-quality data moves forward. Several tools have been developed for this exact purpose. Most are installable via Bioconda, and they can be used independently or as part of an automated pipeline. Here‚Äôs a breakdown of what each tool does: Seqkit: Provides basic statistics about your FASTQ files (e.g., length distribution, GC content). FastQC: Generates per-base quality score plots to detect poor-quality cycles. MultiQC: Aggregates FastQC outputs across samples into a single report. BBMap / Trimmomatic: Trim adapters, remove artifacts, and perform quality filtering. Kneaddata: Specialized for metagenomics, it removes contaminant reads (e.g., host DNA) using alignment-based filtering. 1.2 Shell Code # Install individual tools using mamba and bioconda mamba install -c bioconda seqkit fastqc multiqc bbmap trimmomatic # Install kneaddata from Biobakery channel (for metagenomics) mamba install -c biobakery kneaddata 1.3 R Note # These tools are primarily used from the command line, but their output files # (e.g., FastQC or MultiQC reports) can be imported into R for downstream summarization. "],["how-do-you-obtain-example-microbiome-sequencing-data-for-analysis.html", "Q&A 2 How do you obtain example microbiome sequencing data for analysis? 2.1 Explanation 2.2 Shell Code 2.3 Python Note 2.4 R Note", " Q&A 2 How do you obtain example microbiome sequencing data for analysis? 2.1 Explanation Before performing any analysis, you need access to microbiome sequencing data. This data typically comes in the form of FASTQ files (either single-end or paired-end), which contain raw reads from amplicon sequencing. There are several sources for publicly available datasets: - QIIME2 Tutorials: Include curated sample data for testing pipelines - NCBI SRA / EBI ENA: Provide raw sequencing data from published studies - Qiita: A microbiome database for submitting and reusing 16S/18S/ITS data - Mock communities: Simulated or synthetic datasets used to benchmark tools This example uses the classic Moving Pictures tutorial dataset from QIIME2. 2.2 Shell Code # Download paired-end FASTQ data from QIIME2 tutorial wget https://data.qiime2.org/2024.2/tutorials/moving-pictures/emp-paired-end-sequences/barcodes.fastq.gz wget https://data.qiime2.org/2024.2/tutorials/moving-pictures/emp-paired-end-sequences/forward.fastq.gz wget https://data.qiime2.org/2024.2/tutorials/moving-pictures/emp-paired-end-sequences/reverse.fastq.gz 2.3 Python Note # Although QIIME2 is Python-based, raw sequencing data is usually downloaded externally. # Python/QIIME2 will be used later to import and process these FASTQ files. 2.4 R Note # Most raw sequencing workflows do not begin in R. However, after generating feature tables # from tools like QIIME2 or mothur, R will be used for downstream analysis and visualization. "],["how-do-you-process-raw-sequencing-data-into-a-feature-table-using-qiime2.html", "Q&A 3 How do you process raw sequencing data into a feature table using QIIME2? 3.1 Explanation 3.2 Shell Code (QIIME2 CLI) 3.3 Python Note", " Q&A 3 How do you process raw sequencing data into a feature table using QIIME2? 3.1 Explanation After obtaining your raw FASTQ data, the first analytical step is transforming it into a structured format for analysis ‚Äî the feature table. This is a matrix of counts (samples √ó ASVs or OTUs). QIIME2 is a powerful, Python-based platform for this entire workflow. It uses .qza files (QIIME artifacts) to structure data at each step and generates a .qzv summary for inspection. This pipeline includes: - Importing FASTQ data - Demultiplexing reads - Denoising to generate ASVs using DADA2 or Deblur - Creating a feature table 3.2 Shell Code (QIIME2 CLI) # 1. Import paired-end reads qiime tools import \\ --type EMPPairedEndSequences \\ --input-path emp-paired-end-sequences \\ --output-path emp-paired-end-sequences.qza # 2. Demultiplex (barcode + sample mapping required) qiime demux emp-paired \\ --i-seqs emp-paired-end-sequences.qza \\ --m-barcodes-file sample-metadata.tsv \\ --m-barcodes-column BarcodeSequence \\ --o-per-sample-sequences demux.qza \\ --o-error-correction-details demux-details.qza # 3. Visualize quality qiime demux summarize \\ --i-data demux.qza \\ --o-visualization demux.qzv # 4. Denoise with DADA2 qiime dada2 denoise-paired \\ --i-demultiplexed-seqs demux.qza \\ --p-trim-left-f 0 \\ --p-trim-left-r 0 \\ --p-trunc-len-f 240 \\ --p-trunc-len-r 200 \\ --o-table table.qza \\ --o-representative-sequences rep-seqs.qza \\ --o-denoising-stats denoising-stats.qza 3.3 Python Note # Although QIIME2 is Python-based, its workflow is run via CLI. # Feature table (table.qza) can be exported and summarized later using Python or R. "],["how-do-you-process-raw-sequencing-data-into-a-feature-table-using-mothur.html", "Q&A 4 How do you process raw sequencing data into a feature table using Mothur? 4.1 Explanation 4.2 Shell Code 4.3 R Note 4.4 Python Note", " Q&A 4 How do you process raw sequencing data into a feature table using Mothur? 4.1 Explanation Mothur is an open-source software package for analyzing 16S rRNA gene sequences. It supports raw data preprocessing, OTU clustering, and taxonomic assignment. Its workflow is particularly popular for microbial community studies and works well with single- or paired-end FASTQ data. Mothur pipelines are usually run using command files or interactively within the Mothur console. Key steps include: - Merging paired-end reads - Quality filtering - Aligning and clustering reads into OTUs - Generating .shared (feature table) and .taxonomy files 4.2 Shell Code # Launch Mothur mothur # Inside Mothur console (example workflow) mothur &gt; make.file(inputdir=./raw_data, type=fastq, prefix=stability) mothur &gt; make.contigs(file=stability.files, processors=8) mothur &gt; screen.seqs(fasta=stability.trim.contigs.fasta, group=stability.contigs.groups, maxambig=0, maxlength=275) mothur &gt; unique.seqs(fasta=stability.trim.contigs.good.fasta) mothur &gt; count.seqs(name=stability.trim.contigs.good.names, group=stability.contigs.good.groups) mothur &gt; align.seqs(fasta=stability.trim.contigs.good.unique.fasta, reference=silva.seed_v138.align) mothur &gt; screen.seqs(...) mothur &gt; pre.cluster(...) mothur &gt; chimera.vsearch(...) mothur &gt; remove.seqs(...) mothur &gt; cluster.split(...) mothur &gt; make.shared(...) mothur &gt; classify.otu(...) 4.3 R Note # Mothur outputs a `.shared` file (feature/OTU table) and `.taxonomy` file. # These can be imported into R using packages like phyloseq or tidyverse for analysis. 4.4 Python Note # Mothur is not Python-based, but output files can be parsed with pandas or biom-format readers. "],["how-do-you-explore-and-summarize-a-microbiome-otu-table.html", "Q&A 5 How do you explore and summarize a microbiome OTU table? 5.1 Explanation 5.2 Python Code 5.3 R Code", " Q&A 5 How do you explore and summarize a microbiome OTU table? 5.1 Explanation After generating an OTU (or feature) table from raw sequencing data, it‚Äôs essential to inspect and summarize it before moving into alpha or beta diversity analysis. The OTU table is typically a matrix of samples √ó features (ASVs/OTUs), where each cell contains the abundance count of a feature in a sample. Key summary steps include: - Calculating sample richness (how many OTUs each sample contains) - Measuring OTU prevalence (in how many samples each OTU occurs) - Assessing abundance distribution (e.g., sparse vs dominant OTUs) - Identifying sparse or noisy features that may need filtering 5.2 Python Code import pandas as pd # Load OTU table (OTUs as rows, samples as columns) otu_df = pd.read_csv(&quot;data/otu_table.tsv&quot;, sep=&quot;\\t&quot;, index_col=0) # Number of OTUs per sample (richness) sample_richness = (otu_df &gt; 0).sum(axis=0) # Number of samples per OTU (prevalence) otu_prevalence = (otu_df &gt; 0).sum(axis=1) # Distribution of total counts per OTU otu_abundance_summary = otu_df.sum(axis=1).describe() print(&quot;Sample Richness:&quot;, sample_richness.head()) print(&quot;OTU Prevalence:&quot;, otu_prevalence.head()) print(&quot;Abundance Summary:&quot;, otu_abundance_summary) 5.3 R Code otu_df &lt;- read.delim(&quot;data/otu_table.tsv&quot;, row.names = 1) # Sample richness: number of OTUs per sample colSums(otu_df &gt; 0) Sample_1 Sample_2 Sample_3 Sample_4 Sample_5 Sample_6 Sample_7 Sample_8 44 44 43 40 40 42 45 43 Sample_9 Sample_10 45 41 # OTU prevalence: number of samples each OTU appears in rowSums(otu_df &gt; 0) OTU_1 OTU_2 OTU_3 OTU_4 OTU_5 OTU_6 OTU_7 OTU_8 OTU_9 OTU_10 OTU_11 10 7 9 9 8 8 7 8 9 8 10 OTU_12 OTU_13 OTU_14 OTU_15 OTU_16 OTU_17 OTU_18 OTU_19 OTU_20 OTU_21 OTU_22 9 10 9 9 7 9 10 9 10 7 8 OTU_23 OTU_24 OTU_25 OTU_26 OTU_27 OTU_28 OTU_29 OTU_30 OTU_31 OTU_32 OTU_33 8 9 8 9 9 10 8 8 7 9 9 OTU_34 OTU_35 OTU_36 OTU_37 OTU_38 OTU_39 OTU_40 OTU_41 OTU_42 OTU_43 OTU_44 8 10 10 9 10 7 8 8 9 9 8 OTU_45 OTU_46 OTU_47 OTU_48 OTU_49 OTU_50 8 8 8 7 7 9 # Distribution of total counts per OTU summary(rowSums(otu_df)) Min. 1st Qu. Median Mean 3rd Qu. Max. 36.0 41.0 48.0 47.4 52.0 64.0 "],["how-do-you-filter-out-low-abundance-or-low-prevalence-otus.html", "Q&A 6 How do you filter out low-abundance or low-prevalence OTUs? 6.1 Explanation 6.2 Python Code 6.3 R Code", " Q&A 6 How do you filter out low-abundance or low-prevalence OTUs? 6.1 Explanation OTU tables are often sparse, with many OTUs occurring in only a few samples or at very low abundances. These low-abundance and low-prevalence OTUs can introduce noise, inflate diversity metrics, and complicate downstream analysis. Filtering such OTUs is a critical EDA step before diversity analysis or visualization. Common criteria include: - Prevalence: Removing OTUs that appear in fewer than X samples - Abundance: Removing OTUs with total counts below a threshold This step helps reduce dimensionality and improves interpretability. 6.2 Python Code import pandas as pd # Load OTU table otu_df = pd.read_csv(&quot;data/otu_table.tsv&quot;, sep=&quot;\\t&quot;, index_col=0) # Filter: keep OTUs present in at least 3 samples otu_filtered = otu_df[(otu_df &gt; 0).sum(axis=1) &gt;= 3] # Further filter: keep OTUs with total count ‚â• 10 otu_filtered = otu_filtered[otu_filtered.sum(axis=1) &gt;= 10] # Save filtered table otu_filtered.to_csv(&quot;data/otu_table_filtered.tsv&quot;, sep=&quot;\\t&quot;) 6.3 R Code otu_df &lt;- read.delim(&quot;data/otu_table.tsv&quot;, row.names = 1) # Filter OTUs with prevalence ‚â• 3 samples keep_rows &lt;- rowSums(otu_df &gt; 0) &gt;= 3 otu_df &lt;- otu_df[keep_rows, ] # Further filter by total abundance ‚â• 10 keep_abundant &lt;- rowSums(otu_df) &gt;= 10 otu_df_filtered &lt;- otu_df[keep_abundant, ] # Write filtered table write.table(otu_df_filtered, file = &quot;data/otu_table_filtered.tsv&quot;, sep = &quot;\\t&quot;) "],["how-do-you-visualize-total-otu-abundance-per-sample.html", "Q&A 7 How do you visualize total OTU abundance per sample? 7.1 Explanation 7.2 Python Code 7.3 R Code", " Q&A 7 How do you visualize total OTU abundance per sample? 7.1 Explanation Before diving into deeper microbiome comparisons, it‚Äôs helpful to visualize the sequencing depth ‚Äî the total number of OTU counts per sample. This allows you to check: - Sample variability - Potential outliers - Overall library size distribution Using modern tools like ggplot2 in R or seaborn in Python helps create clearer, more elegant plots. 7.2 Python Code import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Load OTU table otu_df = pd.read_csv(&quot;data/otu_table_filtered.tsv&quot;, sep=&quot;\\t&quot;, index_col=0) # Prepare data total_counts = otu_df.sum(axis=0).reset_index() total_counts.columns = [&quot;Sample&quot;, &quot;Total_OTUs&quot;] # Plot plt.figure(figsize=(10, 5)) sns.barplot(data=total_counts, x=&quot;Sample&quot;, y=&quot;Total_OTUs&quot;, palette=&quot;viridis&quot;) plt.title(&quot;Total OTU Abundance Per Sample&quot;) plt.xticks(rotation=45) plt.ylabel(&quot;Total OTU Counts&quot;) plt.tight_layout() plt.show() 7.3 R Code library(tidyverse) otu_df &lt;- read.delim(&quot;data/otu_table_filtered.tsv&quot;, row.names = 1) otu_long &lt;- colSums(otu_df) %&gt;% enframe(name = &quot;Sample&quot;, value = &quot;Total_OTUs&quot;) ggplot(otu_long, aes(x = Sample, y = Total_OTUs)) + geom_col(fill = &quot;#0073C2FF&quot;) + labs(title = &quot;Total OTU Abundance Per Sample&quot;, y = &quot;Total OTU Counts&quot;) + theme_minimal(base_size = 13) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) "],["how-do-you-create-a-stacked-bar-plot-of-top-genera-across-samples.html", "Q&A 8 How do you create a stacked bar plot of top genera across samples? 8.1 Explanation 8.2 Python Code 8.3 R Code", " Q&A 8 How do you create a stacked bar plot of top genera across samples? 8.1 Explanation Stacked bar plots are widely used in microbiome studies to show relative abundance of microbial taxa across samples. This visual helps assess: - Community composition - Dominant vs rare genera - Variability between sample groups Here we simulate a relative abundance plot using the Genus column from the taxonomy file merged with the OTU table. 8.2 Python Code import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Load OTU table and taxonomy otu_df = pd.read_csv(&quot;data/otu_table_filtered.tsv&quot;, sep=&quot;\\t&quot;, index_col=0) tax_df = pd.read_csv(&quot;data/otu_taxonomy.tsv&quot;) # Merge taxonomy info (Genus) with OTU table merged = otu_df.merge(tax_df[[&quot;OTU_ID&quot;, &quot;Genus&quot;]], left_index=True, right_on=&quot;OTU_ID&quot;) melted = merged.drop(&quot;OTU_ID&quot;, axis=1).melt(id_vars=&quot;Genus&quot;, var_name=&quot;Sample&quot;, value_name=&quot;Abundance&quot;) # Summarize top 8 genera, lump rest as &#39;Other&#39; top_genera = melted.groupby(&quot;Genus&quot;)[&quot;Abundance&quot;].sum().nlargest(8).index melted[&quot;Genus&quot;] = melted[&quot;Genus&quot;].where(melted[&quot;Genus&quot;].isin(top_genera), &quot;Other&quot;) # Normalize per sample melted = melted.groupby([&quot;Sample&quot;, &quot;Genus&quot;])[&quot;Abundance&quot;].sum().reset_index() melted[&quot;RelativeAbundance&quot;] = melted.groupby(&quot;Sample&quot;)[&quot;Abundance&quot;].transform(lambda x: x / x.sum()) # Plot plt.figure(figsize=(12, 5)) sns.barplot(data=melted, x=&quot;Sample&quot;, y=&quot;RelativeAbundance&quot;, hue=&quot;Genus&quot;) plt.title(&quot;Stacked Barplot of Top Genera Across Samples&quot;) plt.ylabel(&quot;Relative Abundance&quot;) plt.xticks(rotation=45) plt.legend(bbox_to_anchor=(1.05, 1), loc=&quot;upper left&quot;) plt.tight_layout() plt.show() 8.3 R Code library(tidyverse) # Load OTU and taxonomy tables otu_df &lt;- read.delim(&quot;data/otu_table_filtered.tsv&quot;, row.names = 1) tax_df &lt;- read.delim(&quot;data/otu_taxonomy.tsv&quot;) # Merge by rownames (OTUs) otu_df$OTU_ID &lt;- rownames(otu_df) merged_df &lt;- left_join(otu_df, tax_df, by = &quot;OTU_ID&quot;) # Convert to long format and summarize long_df &lt;- merged_df %&gt;% pivot_longer(cols = starts_with(&quot;Sample&quot;), names_to = &quot;Sample&quot;, values_to = &quot;Abundance&quot;) %&gt;% group_by(Sample, Genus) %&gt;% summarise(Abundance = sum(Abundance), .groups = &quot;drop&quot;) %&gt;% group_by(Sample) %&gt;% mutate(RelativeAbundance = Abundance / sum(Abundance)) # Keep top 8 genera top_genera &lt;- long_df %&gt;% group_by(Genus) %&gt;% summarise(Total = sum(RelativeAbundance), .groups = &quot;drop&quot;) %&gt;% top_n(8, Total) %&gt;% pull(Genus) long_df &lt;- long_df %&gt;% mutate(Genus = if_else(Genus %in% top_genera, Genus, &quot;Other&quot;)) # Plot ggplot(long_df, aes(x = Sample, y = RelativeAbundance, fill = Genus)) + geom_col() + theme_minimal(base_size = 13) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + labs(title = &quot;Stacked Barplot of Top Genera Across Samples&quot;, y = &quot;Relative Abundance&quot;) "],["how-do-you-visualize-alpha-diversity-richness-across-groups.html", "Q&A 9 How do you visualize alpha diversity (richness) across groups? 9.1 Explanation 9.2 Python Code 9.3 R Code", " Q&A 9 How do you visualize alpha diversity (richness) across groups? 9.1 Explanation Alpha diversity measures within-sample diversity ‚Äî often captured by the number of observed OTUs or ASVs (richness). Visualizing alpha diversity across experimental groups (e.g., Control vs Treatment) helps detect differences in microbial complexity. Boxplots are commonly used for this purpose. To generate the plot, we: - Sum OTUs per sample - Merge with metadata - Group by condition (e.g., Treatment group) 9.2 Python Code import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Load OTU table and metadata otu_df = pd.read_csv(&quot;data/otu_table_filtered.tsv&quot;, sep=&quot;\\t&quot;, index_col=0) meta_df = pd.read_csv(&quot;data/sample_metadata.tsv&quot;, sep=&quot;\\t&quot;) # Compute richness richness = pd.DataFrame({ &quot;sample_id&quot;: otu_df.columns, &quot;richness&quot;: (otu_df &gt; 0).sum(axis=0).values }) # Merge with metadata merged = pd.merge(richness, meta_df, on=&quot;sample_id&quot;) # Plot plt.figure(figsize=(8, 5)) sns.boxplot(data=merged, x=&quot;group&quot;, y=&quot;richness&quot;, palette=&quot;Set2&quot;) sns.stripplot(data=merged, x=&quot;group&quot;, y=&quot;richness&quot;, color=&#39;black&#39;, alpha=0.5) plt.title(&quot;Alpha Diversity (Richness) by Group&quot;) plt.ylabel(&quot;Observed OTUs&quot;) plt.xlabel(&quot;Group&quot;) plt.tight_layout() plt.show() 9.3 R Code library(tidyverse) otu_df &lt;- read.delim(&quot;data/otu_table_filtered.tsv&quot;, row.names = 1) meta_df &lt;- read.delim(&quot;data/sample_metadata.tsv&quot;) # Compute richness richness &lt;- colSums(otu_df &gt; 0) richness_df &lt;- data.frame(sample_id = names(richness), richness = richness) # Merge with metadata merged &lt;- left_join(richness_df, meta_df, by = &quot;sample_id&quot;) # Plot ggplot(merged, aes(x = group, y = richness)) + geom_boxplot(fill = &quot;#00BFC4&quot;, alpha = 0.7) + geom_jitter(width = 0.1, color = &quot;black&quot;, alpha = 0.6) + theme_minimal(base_size = 13) + labs(title = &quot;Alpha Diversity (Richness) by Group&quot;, y = &quot;Observed OTUs&quot;, x = &quot;Group&quot;) "],["how-do-you-perform-ordination-e.g.-pca-to-visualize-sample-clustering.html", "Q&A 10 How do you perform ordination (e.g., PCA) to visualize sample clustering? 10.1 Explanation 10.2 Python Code 10.3 R Code", " Q&A 10 How do you perform ordination (e.g., PCA) to visualize sample clustering? 10.1 Explanation Ordination techniques like PCA, NMDS, or PCoA help reduce the complexity of high-dimensional OTU tables, making it easier to visualize sample relationships. These methods project samples into 2D or 3D based on similarity in microbial composition. Samples that cluster together share similar community profiles. In this example, we‚Äôll perform PCA on centered log-ratio (CLR) transformed data ‚Äî a common preprocessing step in microbiome compositional analysis. 10.2 Python Code import pandas as pd from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler import seaborn as sns import matplotlib.pyplot as plt # Load OTU and metadata otu_df = pd.read_csv(&quot;data/otu_table_filtered.tsv&quot;, sep=&quot;\\t&quot;, index_col=0).T meta_df = pd.read_csv(&quot;data/sample_metadata.tsv&quot;, sep=&quot;\\t&quot;) # Replace 0s with pseudocount for log transformation otu_df += 1 otu_log = np.log(otu_df) # Standardize (optional) otu_scaled = StandardScaler().fit_transform(otu_log) # PCA pca = PCA(n_components=2) pca_result = pca.fit_transform(otu_scaled) pca_df = pd.DataFrame(pca_result, columns=[&quot;PC1&quot;, &quot;PC2&quot;]) pca_df[&quot;sample_id&quot;] = otu_df.index # Merge with metadata pca_df = pd.merge(pca_df, meta_df, on=&quot;sample_id&quot;) # Plot plt.figure(figsize=(8, 6)) sns.scatterplot(data=pca_df, x=&quot;PC1&quot;, y=&quot;PC2&quot;, hue=&quot;group&quot;, style=&quot;location&quot;, s=100) plt.title(&quot;PCA of Microbiome Samples&quot;) plt.xlabel(f&quot;PC1 ({pca.explained_variance_ratio_[0]:.2%})&quot;) plt.ylabel(f&quot;PC2 ({pca.explained_variance_ratio_[1]:.2%})&quot;) plt.tight_layout() plt.show() 10.3 R Code library(tidyverse) library(vegan) otu_df &lt;- read.delim(&quot;data/otu_table_filtered.tsv&quot;, row.names = 1) meta_df &lt;- read.delim(&quot;data/sample_metadata.tsv&quot;) # CLR transformation (log + pseudocount) otu_clr &lt;- log(otu_df + 1) otu_scaled &lt;- scale(t(otu_clr)) # samples as rows # PCA pca_res &lt;- prcomp(otu_scaled, center = TRUE, scale. = TRUE) pca_df &lt;- as.data.frame(pca_res$x[, 1:2]) pca_df$sample_id &lt;- rownames(pca_df) # Merge merged &lt;- left_join(pca_df, meta_df, by = &quot;sample_id&quot;) # Plot ggplot(merged, aes(x = PC1, y = PC2, color = group, shape = location)) + geom_point(size = 3, alpha = 0.8) + theme_minimal(base_size = 13) + labs(title = &quot;PCA of Microbiome Samples&quot;, x = &quot;PC1&quot;, y = &quot;PC2&quot;) "],["how-do-you-visualize-otu-or-genus-abundance-using-a-heatmap.html", "Q&A 11 How do you visualize OTU or Genus abundance using a heatmap? 11.1 Explanation 11.2 Python Code 11.3 R Code", " Q&A 11 How do you visualize OTU or Genus abundance using a heatmap? 11.1 Explanation Heatmaps are excellent for visualizing microbial abundance patterns across samples. They help identify: - Co-occurring OTUs or genera - Sample clusters with similar profiles - High- or low-abundance taxa patterns Heatmaps often include clustering on rows (features) and columns (samples), with scaling or log-transformation to improve interpretability. In this example, we visualize the top 20 most abundant OTUs across all samples. 11.2 Python Code import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Load OTU table otu_df = pd.read_csv(&quot;data/otu_table_filtered.tsv&quot;, sep=&quot;\\t&quot;, index_col=0) # Select top 20 OTUs by total abundance top_otus = otu_df.sum(axis=1).nlargest(20).index top_otu_df = otu_df.loc[top_otus] # Normalize (relative abundance per sample) rel_abund = top_otu_df.div(top_otu_df.sum(axis=0), axis=1) # Plot heatmap plt.figure(figsize=(10, 8)) sns.heatmap(rel_abund, cmap=&quot;YlGnBu&quot;, linewidths=0.5) plt.title(&quot;Heatmap of Top 20 OTUs (Relative Abundance)&quot;) plt.xlabel(&quot;Samples&quot;) plt.ylabel(&quot;OTUs&quot;) plt.tight_layout() plt.show() 11.3 R Code library(tidyverse) library(pheatmap) otu_df &lt;- read.delim(&quot;data/otu_table_filtered.tsv&quot;, row.names = 1) # Select top 20 OTUs by abundance top_otus &lt;- rowSums(otu_df) %&gt;% sort(decreasing = TRUE) %&gt;% head(20) %&gt;% names() top_otu_df &lt;- otu_df[top_otus, ] # Convert to relative abundance rel_abund &lt;- sweep(top_otu_df, 2, colSums(top_otu_df), FUN = &quot;/&quot;) # Plot heatmap pheatmap(rel_abund, color = colorRampPalette(c(&quot;white&quot;, &quot;#0073C2FF&quot;))(100), fontsize = 11, main = &quot;Heatmap of Top 20 OTUs (Relative Abundance)&quot;) "],["how-do-you-statistically-compare-otu-richness-between-groups.html", "Q&A 12 How do you statistically compare OTU richness between groups? 12.1 Explanation 12.2 Python Code 12.3 R Code", " Q&A 12 How do you statistically compare OTU richness between groups? 12.1 Explanation Once you‚Äôve calculated alpha diversity (e.g., observed OTUs per sample), it‚Äôs common to test whether groups differ significantly. This can help you determine if an experimental condition affects microbial richness. Typical tests include: - T-test (for two groups, normally distributed data) - Wilcoxon rank-sum test (non-parametric) - ANOVA or Kruskal-Wallis (for 3+ groups) Here we demonstrate group comparison for richness using appropriate statistical tests. 12.2 Python Code import pandas as pd from scipy.stats import ttest_ind, mannwhitneyu # Load richness and metadata otu_df = pd.read_csv(&quot;data/otu_table_filtered.tsv&quot;, sep=&quot;\\t&quot;, index_col=0) meta_df = pd.read_csv(&quot;data/sample_metadata.tsv&quot;, sep=&quot;\\t&quot;) # Compute richness richness = pd.DataFrame({ &quot;sample_id&quot;: otu_df.columns, &quot;richness&quot;: (otu_df &gt; 0).sum(axis=0).values }) data = pd.merge(richness, meta_df, on=&quot;sample_id&quot;) # Split by group control = data[data[&quot;group&quot;] == &quot;Control&quot;][&quot;richness&quot;] treatment = data[data[&quot;group&quot;] == &quot;Treatment&quot;][&quot;richness&quot;] # T-test (assumes normality) t_stat, t_pval = ttest_ind(control, treatment) # Wilcoxon (non-parametric) w_stat, w_pval = mannwhitneyu(control, treatment) print(f&quot;T-test p-value: {t_pval:.4f}&quot;) print(f&quot;Wilcoxon test p-value: {w_pval:.4f}&quot;) 12.3 R Code library(tidyverse) otu_df &lt;- read.delim(&quot;data/otu_table_filtered.tsv&quot;, row.names = 1) meta_df &lt;- read.delim(&quot;data/sample_metadata.tsv&quot;) # Calculate richness richness &lt;- colSums(otu_df &gt; 0) richness_df &lt;- data.frame(sample_id = names(richness), richness = richness) merged &lt;- left_join(richness_df, meta_df, by = &quot;sample_id&quot;) # T-test t_test &lt;- t.test(richness ~ group, data = merged) # Wilcoxon test wilcox_test &lt;- wilcox.test(richness ~ group, data = merged) t_test$p.value [1] 0.2666018 wilcox_test$p.value [1] 0.2447901 "],["how-do-you-test-for-correlation-between-alpha-diversity-and-age.html", "Q&A 13 How do you test for correlation between alpha diversity and age? 13.1 Explanation 13.2 Python Code 13.3 R Code", " Q&A 13 How do you test for correlation between alpha diversity and age? 13.1 Explanation In many studies, you may want to examine whether microbial diversity is associated with continuous metadata like age, BMI, or pH. Correlation tests help assess linear or monotonic relationships between variables: - Pearson correlation: for linear relationships (assumes normality) - Spearman correlation: for monotonic (rank-based) associations (non-parametric) This Q&amp;A demonstrates testing correlation between richness and age. 13.2 Python Code import pandas as pd from scipy.stats import pearsonr, spearmanr # Load OTU and metadata otu_df = pd.read_csv(&quot;data/otu_table_filtered.tsv&quot;, sep=&quot;\\t&quot;, index_col=0) meta_df = pd.read_csv(&quot;data/sample_metadata.tsv&quot;, sep=&quot;\\t&quot;) # Compute richness richness = pd.DataFrame({ &quot;sample_id&quot;: otu_df.columns, &quot;richness&quot;: (otu_df &gt; 0).sum(axis=0).values }) data = pd.merge(richness, meta_df, on=&quot;sample_id&quot;) # Pearson correlation pearson_corr, pearson_pval = pearsonr(data[&quot;richness&quot;], data[&quot;age&quot;]) # Spearman correlation spearman_corr, spearman_pval = spearmanr(data[&quot;richness&quot;], data[&quot;age&quot;]) print(f&quot;Pearson r: {pearson_corr:.3f}, p = {pearson_pval:.4f}&quot;) print(f&quot;Spearman rho: {spearman_corr:.3f}, p = {spearman_pval:.4f}&quot;) 13.3 R Code library(tidyverse) otu_df &lt;- read.delim(&quot;data/otu_table_filtered.tsv&quot;, row.names = 1) meta_df &lt;- read.delim(&quot;data/sample_metadata.tsv&quot;) # Compute richness richness &lt;- colSums(otu_df &gt; 0) richness_df &lt;- data.frame(sample_id = names(richness), richness = richness) merged &lt;- left_join(richness_df, meta_df, by = &quot;sample_id&quot;) # Pearson correlation cor.test(merged$richness, merged$age, method = &quot;pearson&quot;) Pearson&#39;s product-moment correlation data: merged$richness and merged$age t = -0.099992, df = 8, p-value = 0.9228 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: -0.6504867 0.6078167 sample estimates: cor -0.03533044 # Spearman correlation cor.test(merged$richness, merged$age, method = &quot;spearman&quot;) Spearman&#39;s rank correlation rho data: merged$richness and merged$age S = 179.17, p-value = 0.8135 alternative hypothesis: true rho is not equal to 0 sample estimates: rho -0.08589604 "],["how-do-you-compare-alpha-diversity-across-3-or-more-groups.html", "Q&A 14 How do you compare alpha diversity across 3 or more groups? 14.1 Explanation 14.2 Python Code 14.3 R Code", " Q&A 14 How do you compare alpha diversity across 3 or more groups? 14.1 Explanation When comparing microbial richness across more than two groups, you can use: - ANOVA: if data are normally distributed - Kruskal-Wallis test: non-parametric alternative This Q&amp;A tests whether OTU richness differs by body location (e.g., gut, skin). 14.2 Python Code import pandas as pd from scipy.stats import f_oneway, kruskal # Load data otu_df = pd.read_csv(&quot;data/otu_table_filtered.tsv&quot;, sep=&quot;\\t&quot;, index_col=0) meta_df = pd.read_csv(&quot;data/sample_metadata.tsv&quot;, sep=&quot;\\t&quot;) # Compute richness richness = pd.DataFrame({ &quot;sample_id&quot;: otu_df.columns, &quot;richness&quot;: (otu_df &gt; 0).sum(axis=0).values }) data = pd.merge(richness, meta_df, on=&quot;sample_id&quot;) # Split richness by location groups = [group[&quot;richness&quot;].values for name, group in data.groupby(&quot;location&quot;)] # ANOVA f_stat, f_pval = f_oneway(*groups) # Kruskal-Wallis kw_stat, kw_pval = kruskal(*groups) print(f&quot;ANOVA p-value: {f_pval:.4f}&quot;) print(f&quot;Kruskal-Wallis p-value: {kw_pval:.4f}&quot;) 14.3 R Code library(tidyverse) otu_df &lt;- read.delim(&quot;data/otu_table_filtered.tsv&quot;, row.names = 1) meta_df &lt;- read.delim(&quot;data/sample_metadata.tsv&quot;) # Compute richness richness &lt;- colSums(otu_df &gt; 0) richness_df &lt;- data.frame(sample_id = names(richness), richness = richness) merged &lt;- left_join(richness_df, meta_df, by = &quot;sample_id&quot;) # ANOVA anova_res &lt;- aov(richness ~ location, data = merged) # Kruskal-Wallis kruskal_res &lt;- kruskal.test(richness ~ location, data = merged) summary(anova_res) Df Sum Sq Mean Sq F value Pr(&gt;F) location 1 2.5 2.5 0.676 0.435 Residuals 8 29.6 3.7 kruskal_res Kruskal-Wallis rank sum test data: richness by location Kruskal-Wallis chi-squared = 0.71553, df = 1, p-value = 0.3976 "],["how-do-you-test-for-differences-in-community-composition-using-permanova.html", "Q&A 15 How do you test for differences in community composition using PERMANOVA? 15.1 Explanation 15.2 Python Code 15.3 R Code", " Q&A 15 How do you test for differences in community composition using PERMANOVA? 15.1 Explanation PERMANOVA (Permutational Multivariate Analysis of Variance) is used to test whether beta diversity significantly differs between groups. It operates on a dissimilarity matrix (e.g., Bray-Curtis) and partitions variation based on experimental factors like treatment group or location. This Q&amp;A applies PERMANOVA to Bray-Curtis distances computed from OTU abundances. 15.2 Python Code import pandas as pd from skbio.diversity import beta_diversity from skbio.stats.distance import permanova from skbio import DistanceMatrix # Load data otu_df = pd.read_csv(&quot;data/otu_table_filtered.tsv&quot;, sep=&quot;\\t&quot;, index_col=0).T meta_df = pd.read_csv(&quot;data/sample_metadata.tsv&quot;, sep=&quot;\\t&quot;) # Compute Bray-Curtis distance matrix bray_dm = beta_diversity(&quot;braycurtis&quot;, otu_df.values, ids=otu_df.index) # Format metadata meta_df = meta_df.set_index(&quot;sample_id&quot;).loc[otu_df.index] # Run PERMANOVA result = permanova(distance_matrix=bray_dm, grouping=meta_df[&quot;group&quot;], permutations=999) print(result) 15.3 R Code library(vegan) library(tidyverse) otu_df &lt;- read.delim(&quot;data/otu_table_filtered.tsv&quot;, row.names = 1) meta_df &lt;- read.delim(&quot;data/sample_metadata.tsv&quot;) # Bray-Curtis distance otu_t &lt;- t(otu_df) bray &lt;- vegdist(otu_t, method = &quot;bray&quot;) # Match metadata meta_df &lt;- meta_df %&gt;% filter(sample_id %in% rownames(otu_t)) %&gt;% column_to_rownames(&quot;sample_id&quot;) # Run PERMANOVA (adonis) adonis_res &lt;- adonis2(bray ~ group, data = meta_df, permutations = 999) adonis_res Permutation test for adonis under reduced model Permutation: free Number of permutations: 999 adonis2(formula = bray ~ group, data = meta_df, permutations = 999) Df SumOfSqs R2 F Pr(&gt;F) Model 1 0.03744 0.08118 0.7068 0.89 Residual 8 0.42380 0.91882 Total 9 0.46124 1.00000 "],["how-do-you-test-for-differential-abundance-of-otus-across-groups.html", "Q&A 16 How do you test for differential abundance of OTUs across groups? 16.1 Explanation 16.2 Python Code 16.3 R Code", " Q&A 16 How do you test for differential abundance of OTUs across groups? 16.1 Explanation Differential abundance analysis identifies OTUs that significantly differ between groups (e.g., Control vs Treatment). While tools like DESeq2 are used for RNA-seq and microbiome count data, simpler methods like: - Wilcoxon tests - t-tests - ANCOM / ALDEx2 (specialized tools) can also be used with filtered OTU data. Here we demonstrate how to test one OTU at a time between groups. 16.2 Python Code import pandas as pd from scipy.stats import mannwhitneyu # Load OTU table and metadata otu_df = pd.read_csv(&quot;data/otu_table_filtered.tsv&quot;, sep=&quot;\\t&quot;, index_col=0) meta_df = pd.read_csv(&quot;data/sample_metadata.tsv&quot;, sep=&quot;\\t&quot;).set_index(&quot;sample_id&quot;) otu_df = otu_df[meta_df.index] # ensure matching # Perform Wilcoxon test for each OTU results = [] for otu in otu_df.index: control = otu_df.loc[otu, meta_df[&quot;group&quot;] == &quot;Control&quot;] treatment = otu_df.loc[otu, meta_df[&quot;group&quot;] == &quot;Treatment&quot;] stat, pval = mannwhitneyu(control, treatment) results.append((otu, pval)) # Convert to DataFrame df_results = pd.DataFrame(results, columns=[&quot;OTU&quot;, &quot;p_value&quot;]) df_results[&quot;adjusted_p&quot;] = df_results[&quot;p_value&quot;] * len(df_results) # Bonferroni df_results.sort_values(&quot;p_value&quot;).head() 16.3 R Code library(tidyverse) otu_df &lt;- read.delim(&quot;data/otu_table_filtered.tsv&quot;, row.names = 1) meta_df &lt;- read.delim(&quot;data/sample_metadata.tsv&quot;) # Ensure sample order matches otu_df &lt;- otu_df[, meta_df$sample_id] # Run Wilcoxon test for each OTU results &lt;- apply(otu_df, 1, function(x) { group &lt;- meta_df$group test &lt;- wilcox.test(x[group == &quot;Control&quot;], x[group == &quot;Treatment&quot;]) return(test$p.value) }) df_results &lt;- data.frame(OTU = rownames(otu_df), p_value = results) df_results$adjusted_p &lt;- p.adjust(df_results$p_value, method = &quot;bonferroni&quot;) head(df_results[order(df_results$p_value), ]) OTU p_value adjusted_p OTU_14 OTU_14 0.01962441 0.9812207 OTU_37 OTU_37 0.05855263 1.0000000 OTU_38 OTU_38 0.11049202 1.0000000 OTU_16 OTU_16 0.13262225 1.0000000 OTU_26 OTU_26 0.16123759 1.0000000 OTU_9 OTU_9 0.19882894 1.0000000 "],["how-do-you-prepare-microbiome-data-for-machine-learning.html", "Q&A 17 How do you prepare microbiome data for machine learning? 17.1 Explanation 17.2 Python Code 17.3 R Note", " Q&A 17 How do you prepare microbiome data for machine learning? 17.1 Explanation Before applying machine learning, you must structure your OTU table and metadata into a form suitable for modeling. Typical steps include: - Filtering: Keep relevant OTUs/features - Merging: Align OTU table with sample metadata - Encoding: Set up group labels (e.g., Control = 0, Treatment = 1) - Splitting: Train-test split to evaluate generalizability This Q&amp;A sets up data for classification. 17.2 Python Code import pandas as pd from sklearn.model_selection import train_test_split # Load data otu_df = pd.read_csv(&quot;data/otu_table_filtered.tsv&quot;, sep=&quot;\\t&quot;, index_col=0).T meta_df = pd.read_csv(&quot;data/sample_metadata.tsv&quot;, sep=&quot;\\t&quot;) # Merge OTU table with metadata by sample data = pd.merge(otu_df, meta_df, left_index=True, right_on=&quot;sample_id&quot;) # Define features (X) and labels (y) X = data[otu_df.columns] # OTU features y = data[&quot;group&quot;].map({&quot;Control&quot;: 0, &quot;Treatment&quot;: 1}) # binary encoding # Split into training/testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Check shape print(&quot;Train shape:&quot;, X_train.shape) print(&quot;Test shape:&quot;, X_test.shape) 17.3 R Note # Most ML pipelines in microbiome analysis are performed in Python. # In R, similar workflows can be built using caret, tidymodels, or mlr3. "],["how-do-you-train-and-evaluate-a-random-forest-classifier-on-microbiome-data.html", "Q&A 18 How do you train and evaluate a Random Forest classifier on microbiome data? 18.1 Explanation 18.2 Python Code 18.3 R Code (caret)", " Q&A 18 How do you train and evaluate a Random Forest classifier on microbiome data? 18.1 Explanation The Random Forest algorithm is a popular and robust model for microbiome classification tasks due to its: - Built-in feature importance - Resistance to overfitting - Non-linear modeling capability This Q&amp;A demonstrates how to: - Train a Random Forest classifier - Evaluate it using accuracy and confusion matrix - Inspect important OTUs 18.2 Python Code import pandas as pd from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score, confusion_matrix from sklearn.model_selection import train_test_split # Load and prepare data otu_df = pd.read_csv(&quot;data/otu_table_filtered.tsv&quot;, sep=&quot;\\t&quot;, index_col=0).T meta_df = pd.read_csv(&quot;data/sample_metadata.tsv&quot;, sep=&quot;\\t&quot;) data = pd.merge(otu_df, meta_df, left_index=True, right_on=&quot;sample_id&quot;) X = data[otu_df.columns] y = data[&quot;group&quot;].map({&quot;Control&quot;: 0, &quot;Treatment&quot;: 1}) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Train Random Forest clf = RandomForestClassifier(n_estimators=100, random_state=42) clf.fit(X_train, y_train) # Predict and evaluate y_pred = clf.predict(X_test) acc = accuracy_score(y_test, y_pred) cm = confusion_matrix(y_test, y_pred) print(f&quot;Accuracy: {acc:.2f}&quot;) print(&quot;Confusion Matrix:&quot;) print(cm) # Top 5 important OTUs feat_imp = pd.Series(clf.feature_importances_, index=X.columns) print(&quot;Top OTUs: &quot;, feat_imp.sort_values(ascending=False).head()) 18.3 R Code (caret) library(tidyverse) library(caret) otu_df &lt;- read.delim(&quot;data/otu_table_filtered.tsv&quot;, row.names = 1) meta_df &lt;- read.delim(&quot;data/sample_metadata.tsv&quot;) otu_df &lt;- otu_df[, meta_df$sample_id] otu_df &lt;- t(otu_df) data &lt;- cbind(as.data.frame(otu_df), group = meta_df$group) # Encode group and split data$group &lt;- as.factor(data$group) set.seed(42) trainIndex &lt;- createDataPartition(data$group, p = .7, list = FALSE) train &lt;- data[trainIndex, ] test &lt;- data[-trainIndex, ] # Train Random Forest rf_model &lt;- train(group ~ ., data = train, method = &quot;rf&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 5)) # Predict and evaluate pred &lt;- predict(rf_model, newdata = test) confusionMatrix(pred, test$group) Confusion Matrix and Statistics Reference Prediction Control Treatment Control 0 1 Treatment 1 0 Accuracy : 0 95% CI : (0, 0.8419) No Information Rate : 0.5 P-Value [Acc &gt; NIR] : 1 Kappa : -1 Mcnemar&#39;s Test P-Value : 1 Sensitivity : 0.0 Specificity : 0.0 Pos Pred Value : 0.0 Neg Pred Value : 0.0 Prevalence : 0.5 Detection Rate : 0.0 Detection Prevalence : 0.5 Balanced Accuracy : 0.0 &#39;Positive&#39; Class : Control "],["how-do-you-build-a-logistic-regression-model-for-microbiome-classification.html", "Q&A 19 How do you build a Logistic Regression model for microbiome classification? 19.1 Explanation 19.2 Python Code 19.3 R Code (caret)", " Q&A 19 How do you build a Logistic Regression model for microbiome classification? 19.1 Explanation Logistic Regression is a foundational classification model useful for: - Binary prediction (e.g., Control vs Treatment) - Interpreting OTU effects via coefficients - Establishing baselines before more complex models This Q&amp;A shows how to train and evaluate a Logistic Regression model. 19.2 Python Code import pandas as pd from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score, confusion_matrix from sklearn.model_selection import train_test_split # Load and prepare data otu_df = pd.read_csv(&quot;data/otu_table_filtered.tsv&quot;, sep=&quot;\\t&quot;, index_col=0).T meta_df = pd.read_csv(&quot;data/sample_metadata.tsv&quot;, sep=&quot;\\t&quot;) data = pd.merge(otu_df, meta_df, left_index=True, right_on=&quot;sample_id&quot;) X = data[otu_df.columns] y = data[&quot;group&quot;].map({&quot;Control&quot;: 0, &quot;Treatment&quot;: 1}) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Train logistic regression model model = LogisticRegression(max_iter=1000, random_state=42) model.fit(X_train, y_train) # Predict and evaluate y_pred = model.predict(X_test) acc = accuracy_score(y_test, y_pred) cm = confusion_matrix(y_test, y_pred) print(f&quot;Accuracy: {acc:.2f}&quot;) print(&quot;Confusion Matrix:&quot;) print(cm) # Inspect top OTUs by absolute coefficient magnitude coef = pd.Series(model.coef_[0], index=X.columns) print(&quot;Top OTUs: &quot;, coef.abs().sort_values(ascending=False).head()) 19.3 R Code (caret) library(tidyverse) library(caret) otu_df &lt;- read.delim(&quot;data/otu_table_filtered.tsv&quot;, row.names = 1) meta_df &lt;- read.delim(&quot;data/sample_metadata.tsv&quot;) otu_df &lt;- otu_df[, meta_df$sample_id] otu_df &lt;- t(otu_df) data &lt;- cbind(as.data.frame(otu_df), group = meta_df$group) # Encode group and split data$group &lt;- as.factor(data$group) set.seed(42) trainIndex &lt;- createDataPartition(data$group, p = .7, list = FALSE) train &lt;- data[trainIndex, ] test &lt;- data[-trainIndex, ] # Train logistic regression lr_model &lt;- train(group ~ ., data = train, method = &quot;glm&quot;, family = &quot;binomial&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 5)) # Predict and evaluate pred &lt;- predict(lr_model, newdata = test) confusionMatrix(pred, test$group) Confusion Matrix and Statistics Reference Prediction Control Treatment Control 1 0 Treatment 0 1 Accuracy : 1 95% CI : (0.1581, 1) No Information Rate : 0.5 P-Value [Acc &gt; NIR] : 0.25 Kappa : 1 Mcnemar&#39;s Test P-Value : NA Sensitivity : 1.0 Specificity : 1.0 Pos Pred Value : 1.0 Neg Pred Value : 1.0 Prevalence : 0.5 Detection Rate : 0.5 Detection Prevalence : 0.5 Balanced Accuracy : 1.0 &#39;Positive&#39; Class : Control "],["how-do-you-train-a-support-vector-machine-svm-for-microbiome-classification.html", "Q&A 20 How do you train a Support Vector Machine (SVM) for microbiome classification? 20.1 Explanation 20.2 Python Code 20.3 R Code (caret)", " Q&A 20 How do you train a Support Vector Machine (SVM) for microbiome classification? 20.1 Explanation Support Vector Machines (SVM) are powerful classifiers for high-dimensional biological data. They find a decision boundary (hyperplane) that maximizes class separation. SVMs are well-suited for microbiome data because: - They can handle many features (OTUs) - Work with kernel functions for non-linear separation - Often perform well with sparse data 20.2 Python Code import pandas as pd from sklearn.svm import SVC from sklearn.metrics import accuracy_score, confusion_matrix from sklearn.model_selection import train_test_split # Load and prepare data otu_df = pd.read_csv(&quot;data/otu_table_filtered.tsv&quot;, sep=&quot;\\t&quot;, index_col=0).T meta_df = pd.read_csv(&quot;data/sample_metadata.tsv&quot;, sep=&quot;\\t&quot;) data = pd.merge(otu_df, meta_df, left_index=True, right_on=&quot;sample_id&quot;) X = data[otu_df.columns] y = data[&quot;group&quot;].map({&quot;Control&quot;: 0, &quot;Treatment&quot;: 1}) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Train SVM model = SVC(kernel=&#39;linear&#39;, probability=True, random_state=42) model.fit(X_train, y_train) # Predict and evaluate y_pred = model.predict(X_test) acc = accuracy_score(y_test, y_pred) cm = confusion_matrix(y_test, y_pred) print(f&quot;Accuracy: {acc:.2f}&quot;) print(&quot;Confusion Matrix:&quot;) print(cm) 20.3 R Code (caret) library(tidyverse) library(caret) otu_df &lt;- read.delim(&quot;data/otu_table_filtered.tsv&quot;, row.names = 1) meta_df &lt;- read.delim(&quot;data/sample_metadata.tsv&quot;) otu_df &lt;- otu_df[, meta_df$sample_id] otu_df &lt;- t(otu_df) data &lt;- cbind(as.data.frame(otu_df), group = meta_df$group) # Encode group and split data$group &lt;- as.factor(data$group) set.seed(42) trainIndex &lt;- createDataPartition(data$group, p = .7, list = FALSE) train &lt;- data[trainIndex, ] test &lt;- data[-trainIndex, ] # Train SVM svm_model &lt;- train(group ~ ., data = train, method = &quot;svmLinear&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 5)) # Predict and evaluate pred &lt;- predict(svm_model, newdata = test) confusionMatrix(pred, test$group) Confusion Matrix and Statistics Reference Prediction Control Treatment Control 0 0 Treatment 1 1 Accuracy : 0.5 95% CI : (0.0126, 0.9874) No Information Rate : 0.5 P-Value [Acc &gt; NIR] : 0.75 Kappa : 0 Mcnemar&#39;s Test P-Value : 1.00 Sensitivity : 0.0 Specificity : 1.0 Pos Pred Value : NaN Neg Pred Value : 0.5 Prevalence : 0.5 Detection Rate : 0.0 Detection Prevalence : 0.0 Balanced Accuracy : 0.5 &#39;Positive&#39; Class : Control "],["how-do-you-apply-gradient-boosting-xgboost-for-microbiome-classification.html", "Q&A 21 How do you apply Gradient Boosting (XGBoost) for microbiome classification? 21.1 Explanation 21.2 Python Code 21.3 R Code (caret + xgboost)", " Q&A 21 How do you apply Gradient Boosting (XGBoost) for microbiome classification? 21.1 Explanation XGBoost is an efficient, scalable gradient boosting algorithm widely used in biological classification tasks. It‚Äôs known for: - High performance on structured/tabular data - Handling of missing values - Built-in feature importance metrics This Q&amp;A shows how to train and evaluate an XGBoost classifier. 21.2 Python Code import pandas as pd from xgboost import XGBClassifier from sklearn.metrics import accuracy_score, confusion_matrix from sklearn.model_selection import train_test_split # Load and prepare data otu_df = pd.read_csv(&quot;data/otu_table_filtered.tsv&quot;, sep=&quot;\\t&quot;, index_col=0).T meta_df = pd.read_csv(&quot;data/sample_metadata.tsv&quot;, sep=&quot;\\t&quot;) data = pd.merge(otu_df, meta_df, left_index=True, right_on=&quot;sample_id&quot;) X = data[otu_df.columns] y = data[&quot;group&quot;].map({&quot;Control&quot;: 0, &quot;Treatment&quot;: 1}) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Train XGBoost classifier model = XGBClassifier(use_label_encoder=False, eval_metric=&#39;logloss&#39;, random_state=42) model.fit(X_train, y_train) # Predict and evaluate y_pred = model.predict(X_test) acc = accuracy_score(y_test, y_pred) cm = confusion_matrix(y_test, y_pred) print(f&quot;Accuracy: {acc:.2f}&quot;) print(&quot;Confusion Matrix:&quot;) print(cm) # Top important OTUs importances = pd.Series(model.feature_importances_, index=X.columns) print(&quot;Top Features: &quot;, importances.sort_values(ascending=False).head()) 21.3 R Code (caret + xgboost) library(tidyverse) library(caret) library(xgboost) otu_df &lt;- read.delim(&quot;data/otu_table_filtered.tsv&quot;, row.names = 1) meta_df &lt;- read.delim(&quot;data/sample_metadata.tsv&quot;) otu_df &lt;- otu_df[, meta_df$sample_id] otu_df &lt;- t(otu_df) data &lt;- cbind(as.data.frame(otu_df), group = meta_df$group) # Encode group and split data$group &lt;- as.factor(data$group) set.seed(42) trainIndex &lt;- createDataPartition(data$group, p = .7, list = FALSE) train &lt;- data[trainIndex, ] test &lt;- data[-trainIndex, ] # Train XGBoost xgb_model &lt;- train(group ~ ., data = train, method = &quot;xgbTree&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 5)) [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:46] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. [18:30:47] WARNING: src/c_api/c_api.cc:935: `ntree_limit` is deprecated, use `iteration_range` instead. # Predict and evaluate pred &lt;- predict(xgb_model, newdata = test) confusionMatrix(pred, test$group) Confusion Matrix and Statistics Reference Prediction Control Treatment Control 1 1 Treatment 0 0 Accuracy : 0.5 95% CI : (0.0126, 0.9874) No Information Rate : 0.5 P-Value [Acc &gt; NIR] : 0.75 Kappa : 0 Mcnemar&#39;s Test P-Value : 1.00 Sensitivity : 1.0 Specificity : 0.0 Pos Pred Value : 0.5 Neg Pred Value : NaN Prevalence : 0.5 Detection Rate : 0.5 Detection Prevalence : 1.0 Balanced Accuracy : 0.5 &#39;Positive&#39; Class : Control "],["how-do-you-visualize-roc-curves-to-compare-classification-models.html", "Q&A 22 How do you visualize ROC curves to compare classification models? 22.1 Explanation 22.2 Python Code 22.3 R Code (caret + pROC)", " Q&A 22 How do you visualize ROC curves to compare classification models? 22.1 Explanation Receiver Operating Characteristic (ROC) curves help visualize model performance across different thresholds. The Area Under the Curve (AUC) summarizes performance ‚Äî closer to 1.0 is better. Comparing ROC curves across models (e.g., Random Forest, Logistic Regression, XGBoost) provides insight into which performs best and where they differ. This Q&amp;A demonstrates ROC curve generation in Python and R. 22.2 Python Code import pandas as pd from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.linear_model import LogisticRegression from xgboost import XGBClassifier from sklearn.metrics import roc_curve, auc import matplotlib.pyplot as plt # Load and prepare data otu_df = pd.read_csv(&quot;data/otu_table_filtered.tsv&quot;, sep=&quot;\\t&quot;, index_col=0).T meta_df = pd.read_csv(&quot;data/sample_metadata.tsv&quot;, sep=&quot;\\t&quot;) data = pd.merge(otu_df, meta_df, left_index=True, right_on=&quot;sample_id&quot;) X = data[otu_df.columns] y = data[&quot;group&quot;].map({&quot;Control&quot;: 0, &quot;Treatment&quot;: 1}) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Define models models = { &quot;Random Forest&quot;: RandomForestClassifier(random_state=42), &quot;Logistic Regression&quot;: LogisticRegression(max_iter=1000), &quot;XGBoost&quot;: XGBClassifier(use_label_encoder=False, eval_metric=&quot;logloss&quot;, random_state=42) } # Plot ROC curves plt.figure(figsize=(8, 6)) for name, model in models.items(): model.fit(X_train, y_train) probas = model.predict_proba(X_test)[:, 1] fpr, tpr, _ = roc_curve(y_test, probas) auc_score = auc(fpr, tpr) plt.plot(fpr, tpr, label=f&quot;{name} (AUC = {auc_score:.2f})&quot;) plt.plot([0, 1], [0, 1], &quot;k--&quot;) plt.xlabel(&quot;False Positive Rate&quot;) plt.ylabel(&quot;True Positive Rate&quot;) plt.title(&quot;ROC Curve Comparison&quot;) plt.legend() plt.tight_layout() plt.show() 22.3 R Code (caret + pROC) library(tidyverse) library(caret) library(pROC) otu_df &lt;- read.delim(&quot;data/otu_table_filtered.tsv&quot;, row.names = 1) meta_df &lt;- read.delim(&quot;data/sample_metadata.tsv&quot;) otu_df &lt;- otu_df[, meta_df$sample_id] otu_df &lt;- t(otu_df) data &lt;- cbind(as.data.frame(otu_df), group = as.factor(meta_df$group)) # Train/test split set.seed(42) trainIndex &lt;- createDataPartition(data$group, p = .7, list = FALSE) train &lt;- data[trainIndex, ] test &lt;- data[-trainIndex, ] # Define fixed tuning for models that require it rf_grid &lt;- data.frame(mtry = floor(sqrt(ncol(train) - 1))) svm_grid &lt;- data.frame(C = 1) xgb_grid &lt;- data.frame( nrounds = 50, max_depth = 3, eta = 0.3, gamma = 0, colsample_bytree = 1, min_child_weight = 1, subsample = 1 ) # Train models using fixed tuneGrid models &lt;- list( rf = train(group ~ ., data = train, method = &quot;rf&quot;, trControl = trainControl(method = &quot;none&quot;), tuneGrid = rf_grid), glm = train(group ~ ., data = train, method = &quot;glm&quot;, family = &quot;binomial&quot;, trControl = trainControl(method = &quot;none&quot;)), svm = train(group ~ ., data = train, method = &quot;svmLinear&quot;, trControl = trainControl(method = &quot;none&quot;), tuneGrid = svm_grid), xgb = train(group ~ ., data = train, method = &quot;xgbTree&quot;, trControl = trainControl(method = &quot;none&quot;), tuneGrid = xgb_grid) ) # ROC analysis roc_list &lt;- lapply(models, function(model) { probs &lt;- predict(model, newdata = test, type = &quot;prob&quot;) # Safely extract numeric probabilities for the &quot;Treatment&quot; class class_label &lt;- &quot;Treatment&quot; if (!(class_label %in% colnames(probs))) { stop(paste(&quot;Class label&quot;, class_label, &quot;not found in predicted probabilities&quot;)) } prob_values &lt;- as.numeric(probs[, class_label]) roc(response = test$group, predictor = prob_values) }) # Plot ROC plot(roc_list[[1]], col = &quot;blue&quot;, legacy.axes = TRUE, main = &quot;ROC Curves - Microbiome Classification&quot;) cols &lt;- c(&quot;blue&quot;, &quot;green&quot;, &quot;red&quot;, &quot;purple&quot;) for (i in 2:length(roc_list)) { plot(roc_list[[i]], col = cols[i], add = TRUE) } legend(&quot;bottomright&quot;, legend = names(models), col = cols, lwd = 2) "],["how-do-you-apply-cross-validation-strategies-to-evaluate-model-reliability.html", "Q&A 23 How do you apply cross-validation strategies to evaluate model reliability? 23.1 Explanation 23.2 Python Code 23.3 R Code (caret with repeated k-fold CV)", " Q&A 23 How do you apply cross-validation strategies to evaluate model reliability? 23.1 Explanation Cross-validation (CV) is critical for evaluating how well a machine learning model generalizes to unseen data. It reduces the risk of overfitting by testing the model on multiple train/test splits. Common CV strategies: - k-Fold: Split into k subsets, rotate test set - Repeated k-Fold: More robust by repeating k-fold several times - Stratified k-Fold: Ensures balanced class distribution in folds This Q&amp;A shows how to apply CV in Python and R using common microbiome classifiers. 23.2 Python Code import pandas as pd from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import cross_val_score, StratifiedKFold # Load and prepare data otu_df = pd.read_csv(&quot;data/otu_table_filtered.tsv&quot;, sep=&quot;\\t&quot;, index_col=0).T meta_df = pd.read_csv(&quot;data/sample_metadata.tsv&quot;, sep=&quot;\\t&quot;) data = pd.merge(otu_df, meta_df, left_index=True, right_on=&quot;sample_id&quot;) X = data[otu_df.columns] y = data[&quot;group&quot;].map({&quot;Control&quot;: 0, &quot;Treatment&quot;: 1}) # Define CV strategy cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42) # Evaluate Random Forest using cross-validation model = RandomForestClassifier(random_state=42) scores = cross_val_score(model, X, y, cv=cv, scoring=&quot;accuracy&quot;) print(&quot;Cross-Validation Accuracy Scores:&quot;, scores) print(&quot;Mean Accuracy:&quot;, scores.mean()) 23.3 R Code (caret with repeated k-fold CV) library(tidyverse) library(caret) otu_df &lt;- read.delim(&quot;data/otu_table_filtered.tsv&quot;, row.names = 1) meta_df &lt;- read.delim(&quot;data/sample_metadata.tsv&quot;) otu_df &lt;- otu_df[, meta_df$sample_id] otu_df &lt;- t(otu_df) data &lt;- cbind(as.data.frame(otu_df), group = as.factor(meta_df$group)) # Define CV control ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 5, repeats = 3) # Train with CV set.seed(42) cv_model &lt;- train(group ~ ., data = data, method = &quot;rf&quot;, trControl = ctrl) # Results print(cv_model) Random Forest 10 samples 50 predictors 2 classes: &#39;Control&#39;, &#39;Treatment&#39; No pre-processing Resampling: Cross-Validated (5 fold, repeated 3 times) Summary of sample sizes: 8, 8, 8, 8, 8, 8, ... Resampling results across tuning parameters: mtry Accuracy Kappa 2 0.2333333 -0.5333333 26 0.4000000 -0.2000000 50 0.4000000 -0.2000000 Accuracy was used to select the optimal model using the largest value. The final value used for the model was mtry = 26. plot(cv_model) "],["how-do-you-use-mikropml-in-r-for-microbiome-machine-learning.html", "Q&A 24 How do you use mikropml in R for microbiome machine learning? 24.1 Explanation 24.2 R Code 24.3 Notes", " Q&A 24 How do you use mikropml in R for microbiome machine learning? 24.1 Explanation mikropml is a microbiome-focused R package by Pat Schloss designed for: - End-to-end modeling workflows - Built-in cross-validation and hyperparameter tuning - Transparency in model reporting and evaluation It simplifies the process of building, tuning, and interpreting microbiome ML models. This Q&amp;A introduces a basic pipeline using mikropml and prepares the OTU and metadata files as expected. 24.2 R Code # üì¶ Ensure mikropml is installed if (!requireNamespace(&quot;mikropml&quot;, quietly = TRUE)) { if (!requireNamespace(&quot;remotes&quot;, quietly = TRUE)) install.packages(&quot;remotes&quot;) remotes::install_github(&quot;SchlossLab/mikropml&quot;) } library(mikropml) library(tidyverse) # Load OTU table and metadata otu &lt;- read.delim(&quot;data/otu_table_filtered.tsv&quot;, row.names = 1) meta &lt;- read.delim(&quot;data/sample_metadata.tsv&quot;) # Transpose OTU so samples are rows otu_t &lt;- t(otu) otu_df &lt;- as.data.frame(otu_t) otu_df$sample_id &lt;- rownames(otu_t) # Merge with metadata data &lt;- inner_join(otu_df, meta, by = &quot;sample_id&quot;) # Run mikropml using run_ml() set.seed(42) fit &lt;- run_ml( dataset = data, outcome_colname = &quot;group&quot;, method = &quot;rf&quot;, # Choose from rf, svm, glmnet, xgb seed = 42 ) # View model summary summary(fit) Length Class Mode trained_model 21 train list test_data 55 data.frame list performance 17 tbl_df list feature_importance 1 -none- character # Plot variable importance fit$importance_plot NULL 24.3 Notes mikropl supports additional tuning and export for reproducibility. mikropml() auto-detects classification vs regression tasks. "],["microbiome-data-analysis-workflow.html", "A Microbiome Data Analysis Workflow", " A Microbiome Data Analysis Workflow "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]

[["index.html", "Microbiome Exploratory Data Analysis ", " Microbiome Exploratory Data Analysis Live site: https://microbiome.complexdatainsights.com Last updated: May 26, 2025 "],["what-are-the-essential-tools-for-microbiome-read-quality-control.html", "Q&A 1 What are the essential tools for microbiome read quality control? 1.1 Explanation 1.2 Shell Code 1.3 R Note", " Q&A 1 What are the essential tools for microbiome read quality control? 1.1 Explanation Every microbiome analysis begins with raw sequencing data, often in the form of FASTQ files. These files contain both the nucleotide reads and quality scores. However, raw reads are rarely perfect — they may contain adapter sequences, low-quality regions, or even contaminant DNA. Before proceeding to any taxonomic or functional profiling, it’s essential to clean and assess these reads. This is the foundation of your analysis pipeline — ensuring that only high-quality data moves forward. Several tools have been developed for this exact purpose. Most are installable via Bioconda, and they can be used independently or as part of an automated pipeline. Here’s a breakdown of what each tool does: Seqkit: Provides basic statistics about your FASTQ files (e.g., length distribution, GC content). FastQC: Generates per-base quality score plots to detect poor-quality cycles. MultiQC: Aggregates FastQC outputs across samples into a single report. BBMap / Trimmomatic: Trim adapters, remove artifacts, and perform quality filtering. Kneaddata: Specialized for metagenomics, it removes contaminant reads (e.g., host DNA) using alignment-based filtering. 1.2 Shell Code # Install individual tools using mamba and bioconda mamba install -c bioconda seqkit fastqc multiqc bbmap trimmomatic # Install kneaddata from Biobakery channel (for metagenomics) mamba install -c biobakery kneaddata 1.3 R Note # These tools are primarily used from the command line, but their output files # (e.g., FastQC or MultiQC reports) can be imported into R for downstream summarization. "],["how-do-i-obtain-example-microbiome-sequencing-data-for-analysis.html", "Q&A 2 How do I obtain example microbiome sequencing data for analysis? 2.1 Explanation 2.2 Shell Code 2.3 Python Note 2.4 R Note", " Q&A 2 How do I obtain example microbiome sequencing data for analysis? 2.1 Explanation Before performing any analysis, you need access to microbiome sequencing data. This data typically comes in the form of FASTQ files (either single-end or paired-end), which contain raw reads from amplicon sequencing. There are several sources for publicly available datasets: - QIIME2 Tutorials: Include curated sample data for testing pipelines - NCBI SRA / EBI ENA: Provide raw sequencing data from published studies - Qiita: A microbiome database for submitting and reusing 16S/18S/ITS data - Mock communities: Simulated or synthetic datasets used to benchmark tools This example uses the classic Moving Pictures tutorial dataset from QIIME2. 2.2 Shell Code # Download paired-end FASTQ data from QIIME2 tutorial wget https://data.qiime2.org/2024.2/tutorials/moving-pictures/emp-paired-end-sequences/barcodes.fastq.gz wget https://data.qiime2.org/2024.2/tutorials/moving-pictures/emp-paired-end-sequences/forward.fastq.gz wget https://data.qiime2.org/2024.2/tutorials/moving-pictures/emp-paired-end-sequences/reverse.fastq.gz 2.3 Python Note # Although QIIME2 is Python-based, raw sequencing data is usually downloaded externally. # Python/QIIME2 will be used later to import and process these FASTQ files. 2.4 R Note # Most raw sequencing workflows do not begin in R. However, after generating feature tables # from tools like QIIME2 or mothur, R will be used for downstream analysis and visualization. "],["how-do-i-process-raw-sequencing-data-into-a-feature-table-using-qiime2.html", "Q&A 3 How do I process raw sequencing data into a feature table using QIIME2? 3.1 Explanation 3.2 Shell Code (QIIME2 CLI) 3.3 Python Note", " Q&A 3 How do I process raw sequencing data into a feature table using QIIME2? 3.1 Explanation After obtaining your raw FASTQ data, the first analytical step is transforming it into a structured format for analysis — the feature table. This is a matrix of counts (samples × ASVs or OTUs). QIIME2 is a powerful, Python-based platform for this entire workflow. It uses .qza files (QIIME artifacts) to structure data at each step and generates a .qzv summary for inspection. This pipeline includes: - Importing FASTQ data - Demultiplexing reads - Denoising to generate ASVs using DADA2 or Deblur - Creating a feature table 3.2 Shell Code (QIIME2 CLI) # 1. Import paired-end reads qiime tools import \\ --type EMPPairedEndSequences \\ --input-path emp-paired-end-sequences \\ --output-path emp-paired-end-sequences.qza # 2. Demultiplex (barcode + sample mapping required) qiime demux emp-paired \\ --i-seqs emp-paired-end-sequences.qza \\ --m-barcodes-file sample-metadata.tsv \\ --m-barcodes-column BarcodeSequence \\ --o-per-sample-sequences demux.qza \\ --o-error-correction-details demux-details.qza # 3. Visualize quality qiime demux summarize \\ --i-data demux.qza \\ --o-visualization demux.qzv # 4. Denoise with DADA2 qiime dada2 denoise-paired \\ --i-demultiplexed-seqs demux.qza \\ --p-trim-left-f 0 \\ --p-trim-left-r 0 \\ --p-trunc-len-f 240 \\ --p-trunc-len-r 200 \\ --o-table table.qza \\ --o-representative-sequences rep-seqs.qza \\ --o-denoising-stats denoising-stats.qza 3.3 Python Note # Although QIIME2 is Python-based, its workflow is run via CLI. # Feature table (table.qza) can be exported and summarized later using Python or R. "],["how-do-i-process-raw-sequencing-data-into-a-feature-table-using-mothur.html", "Q&A 4 How do I process raw sequencing data into a feature table using Mothur? 4.1 Explanation 4.2 Shell Code 4.3 R Note 4.4 Python Note", " Q&A 4 How do I process raw sequencing data into a feature table using Mothur? 4.1 Explanation Mothur is an open-source software package for analyzing 16S rRNA gene sequences. It supports raw data preprocessing, OTU clustering, and taxonomic assignment. Its workflow is particularly popular for microbial community studies and works well with single- or paired-end FASTQ data. Mothur pipelines are usually run using command files or interactively within the Mothur console. Key steps include: - Merging paired-end reads - Quality filtering - Aligning and clustering reads into OTUs - Generating .shared (feature table) and .taxonomy files 4.2 Shell Code # Launch Mothur mothur # Inside Mothur console (example workflow) mothur &gt; make.file(inputdir=./raw_data, type=fastq, prefix=stability) mothur &gt; make.contigs(file=stability.files, processors=8) mothur &gt; screen.seqs(fasta=stability.trim.contigs.fasta, group=stability.contigs.groups, maxambig=0, maxlength=275) mothur &gt; unique.seqs(fasta=stability.trim.contigs.good.fasta) mothur &gt; count.seqs(name=stability.trim.contigs.good.names, group=stability.contigs.good.groups) mothur &gt; align.seqs(fasta=stability.trim.contigs.good.unique.fasta, reference=silva.seed_v138.align) mothur &gt; screen.seqs(...) mothur &gt; pre.cluster(...) mothur &gt; chimera.vsearch(...) mothur &gt; remove.seqs(...) mothur &gt; cluster.split(...) mothur &gt; make.shared(...) mothur &gt; classify.otu(...) 4.3 R Note # Mothur outputs a `.shared` file (feature/OTU table) and `.taxonomy` file. # These can be imported into R using packages like phyloseq or tidyverse for analysis. 4.4 Python Note # Mothur is not Python-based, but output files can be parsed with pandas or biom-format readers. "],["how-do-i-explore-and-summarize-a-microbiome-otu-table.html", "Q&A 5 How do I explore and summarize a microbiome OTU table? 5.1 Explanation 5.2 Python Code 5.3 R Code", " Q&A 5 How do I explore and summarize a microbiome OTU table? 5.1 Explanation After generating an OTU (or feature) table from raw sequencing data, it’s essential to inspect and summarize it before moving into alpha or beta diversity analysis. The OTU table is typically a matrix of samples × features (ASVs/OTUs), where each cell contains the abundance count of a feature in a sample. Key summary steps include: - Calculating sample richness (how many OTUs each sample contains) - Measuring OTU prevalence (in how many samples each OTU occurs) - Assessing abundance distribution (e.g., sparse vs dominant OTUs) - Identifying sparse or noisy features that may need filtering 5.2 Python Code import pandas as pd # Load OTU table (OTUs as rows, samples as columns) otu_df = pd.read_csv(&quot;data/otu_table.tsv&quot;, sep=&quot;\\t&quot;, index_col=0) # Number of OTUs per sample (richness) sample_richness = (otu_df &gt; 0).sum(axis=0) # Number of samples per OTU (prevalence) otu_prevalence = (otu_df &gt; 0).sum(axis=1) # Distribution of total counts per OTU otu_abundance_summary = otu_df.sum(axis=1).describe() print(&quot;Sample Richness:&quot;, sample_richness.head()) print(&quot;OTU Prevalence:&quot;, otu_prevalence.head()) print(&quot;Abundance Summary:&quot;, otu_abundance_summary) 5.3 R Code otu_df &lt;- read.delim(&quot;data/otu_table.tsv&quot;, row.names = 1) # Sample richness: number of OTUs per sample colSums(otu_df &gt; 0) Sample_1 Sample_2 Sample_3 Sample_4 Sample_5 Sample_6 Sample_7 Sample_8 44 44 43 40 40 42 45 43 Sample_9 Sample_10 45 41 # OTU prevalence: number of samples each OTU appears in rowSums(otu_df &gt; 0) OTU_1 OTU_2 OTU_3 OTU_4 OTU_5 OTU_6 OTU_7 OTU_8 OTU_9 OTU_10 OTU_11 10 7 9 9 8 8 7 8 9 8 10 OTU_12 OTU_13 OTU_14 OTU_15 OTU_16 OTU_17 OTU_18 OTU_19 OTU_20 OTU_21 OTU_22 9 10 9 9 7 9 10 9 10 7 8 OTU_23 OTU_24 OTU_25 OTU_26 OTU_27 OTU_28 OTU_29 OTU_30 OTU_31 OTU_32 OTU_33 8 9 8 9 9 10 8 8 7 9 9 OTU_34 OTU_35 OTU_36 OTU_37 OTU_38 OTU_39 OTU_40 OTU_41 OTU_42 OTU_43 OTU_44 8 10 10 9 10 7 8 8 9 9 8 OTU_45 OTU_46 OTU_47 OTU_48 OTU_49 OTU_50 8 8 8 7 7 9 # Distribution of total counts per OTU summary(rowSums(otu_df)) Min. 1st Qu. Median Mean 3rd Qu. Max. 36.0 41.0 48.0 47.4 52.0 64.0 "],["how-do-i-filter-out-low-abundance-or-low-prevalence-otus.html", "Q&A 6 How do I filter out low-abundance or low-prevalence OTUs? 6.1 Explanation 6.2 Python Code 6.3 R Code", " Q&A 6 How do I filter out low-abundance or low-prevalence OTUs? 6.1 Explanation OTU tables are often sparse, with many OTUs occurring in only a few samples or at very low abundances. These low-abundance and low-prevalence OTUs can introduce noise, inflate diversity metrics, and complicate downstream analysis. Filtering such OTUs is a critical EDA step before diversity analysis or visualization. Common criteria include: - Prevalence: Removing OTUs that appear in fewer than X samples - Abundance: Removing OTUs with total counts below a threshold This step helps reduce dimensionality and improves interpretability. 6.2 Python Code import pandas as pd # Load OTU table otu_df = pd.read_csv(&quot;data/otu_table.tsv&quot;, sep=&quot;\\t&quot;, index_col=0) # Filter: keep OTUs present in at least 3 samples otu_filtered = otu_df[(otu_df &gt; 0).sum(axis=1) &gt;= 3] # Further filter: keep OTUs with total count ≥ 10 otu_filtered = otu_filtered[otu_filtered.sum(axis=1) &gt;= 10] # Save filtered table otu_filtered.to_csv(&quot;data/otu_table_filtered.tsv&quot;, sep=&quot;\\t&quot;) 6.3 R Code otu_df &lt;- read.delim(&quot;data/otu_table.tsv&quot;, row.names = 1) # Filter OTUs with prevalence ≥ 3 samples keep_rows &lt;- rowSums(otu_df &gt; 0) &gt;= 3 otu_df &lt;- otu_df[keep_rows, ] # Further filter by total abundance ≥ 10 keep_abundant &lt;- rowSums(otu_df) &gt;= 10 otu_df_filtered &lt;- otu_df[keep_abundant, ] # Write filtered table write.table(otu_df_filtered, file = &quot;data/otu_table_filtered.tsv&quot;, sep = &quot;\\t&quot;) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
